## Systematic review 논문 자동 작성을 위한 파이프라인 구축

##### 2025.10.13 (1일차) 병원 #####

### 00. 사전 조사

1.  Gemma-3 모델(.llamafile) 사용법 스터디 노트

    a. llamafile 이란?
       - llamafile은 대규모 언어 모델(LLM)의 가중치와 실행 코드를 하나로 묶은 단일 실행 파일임.
       - 덕분에 리눅스, macOS, 윈도우 등 다양한 운영체제에서 별도의 설치 과정 없이 모델을 쉽게 실행할 수 있음.

    b. 로컬 모델 실행 방법 (Windows 기준)
       - 로컬에 저장된 모델 파일: `C:\AI_models\google_gemma-3-12b-it-Q4_K_M.llamafile`
       - 기본 실행 (터미널 채팅)
         ```bash
         C:\AI_models\google_gemma-3-12b-it-Q4_K_M.llamafile
         ```
       - GPU 가속 사용 (성능 향상)
         ```bash
         C:\AI_models\google_gemma-3-12b-it-Q4_K_M.llamafile -ngl 999
         ```
       - 웹 UI 사용
         ```bash
         C:\AI_models\google_gemma-3-12b-it-Q4_K_M.llamafile --server
         ```

    c. 기타 유용한 옵션
       - 컨텍스트 크기 조절: `-c` 플래그로 컨텍스트 창 크기를 조절할 수 있음. (예: `-c 0`은 최대 크기 사용)
       - 파일과 대화하기: `-f` 플래그로 텍스트 파일을 지정하면, 해당 파일의 내용을 기반으로 대화를 시작할 수 있음.

    d. 코드에서 API처럼 사용하기 (Python 예제)
       - `llamafile`을 서버 모드로 실행하면, OpenAI API와 호환되는 로컬 API 서버가 생성됨.
       - 1단계: 서버 실행
         ```bash
         C:\AI_models\google_gemma-3-12b-it-Q4_K_M.llamafile --server -ngl 999
         ```
       - 2단계: Python 코드에서 호출
         ```python
         import openai

         client = openai.OpenAI(
             base_url="http://127.0.0.1:8080/v1",
             api_key="sk-no-key-required"
         )

         completion = client.chat.completions.create(
             model="gpt-3.5-turbo",
             messages=[
                 {"role": "system", "content": "You are a helpful assistant."},
                 {"role": "user", "content": "Hello! Can you tell me about Large Language Models?"}
             ]
         )

         print(completion.choices[0].message.content)
         ```

2.  논문 작성 가이드라인 (Cochrane Intervention Review 기반)

    a. 주요 원칙
       - PRISMA 2020 지침 준수
       - 범위 관리: 명확한 연구 질문(PICO) 설정
       - 간결성: 전체 논문 10,000 단어 이내 권장

    b. 논문 핵심 구조
       - 1. 초록 (Abstract)
       - 2. 쉬운 말 요약 (Plain Language Summary)
       - 3. 서론 (Background)
       - 4. 연구 방법 (Methods)
       - 5. 연구 결과 (Results)
       - 6. 고찰 (Discussion)
       - 7. 저자 결론 (Authors' conclusion)

3.  데이터 추출 템플릿 구조 (Example001.csv 기반)

    a. 목적
       - 체계적 문헌고찰 과정에서 각 논문의 핵심 정보를 일관된 형식으로 추출하고 관리함.

    b. 주요 항목 (CSV 컬럼)
       - Title, Author Names, Abstract, DOI, Full Text Link, Embase Link 등

4.  관련 링크
    - Github - Mozilla-Ocho/llamafile: https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file
    - Hugging Face - Mozilla/gemma-3-12b-it-llamafile: https://huggingface.co/Mozilla/gemma-3-12b-it-llamafile
    - How to do a systematic review: https://journals.sagepub.com/doi/full/10.1177/1747493017743796

### 01. 기획 문서 작성 및 초기 환경 구축

1.  프로젝트 구조 생성
    - `sr-gemma3` 프로젝트 디렉터리 구조를 생성함.

2.  오픈소스 도구 복제
    - Git의 긴 파일 경로 문제 해결 (`core.longpaths` 설정) 후, 기획 문서에 명시된 핵심 오픈소스 도구(ASReview, RobotReviewer, GROBID)를 `sr-gemma3/tools/`에 복제함.

3.  의존성 파일 생성
    - Python 의존성 관리를 위한 `requirements.txt` 파일을 생성함 (`openai`, `requests`, `pandas`, `asreview` 등 명시).

4.  초기 코드 스캐폴드 작성
    - SR 파이프라인의 각 기능 모듈에 대한 초기 코드를 작성함.
      - a. `main.py`: 전체 워크플로우를 조율하는 메인 스크립트
      - b. `src/llm/client.py`: 로컬 Gemma 3 (llamafile) 모델과 통신하는 클라이언트
      - c. `src/ingest/pubmed.py`: PubMed API를 통해 논문 데이터를 수집하는 모듈
      - d. `src/parse/grobid_client.py`: PDF 문서를 구조화된 텍스트로 파싱하기 위한 GROBID 클라이언트

### 02. 향후 작업 계획

1.  Python 환경 설정
    - `sr-gemma3` 디렉터리에서 가상 환경을 생성하고, `pip install -r requirements.txt` 명령어로 필요한 라이브러리를 설치함.

2.  핵심 서비스 실행
    - a. Docker: Docker Desktop을 설치하고, `sr-gemma3/tools/grobid` 디렉터리의 가이드를 따라 GROBID 서비스를 컨테이너로 실행함.
    - b. LLM 서버: `study_note.txt`를 참고하여 로컬 Gemma 3 모델(`google_gemma-3-12b-it-Q4_K_M.llamafile`)을 GPU 가속을 적용한 서버 모드로 실행함. (`--server -ngl 999`)

3.  초기 파이프라인 테스트
    - `python sr-gemma3/main.py`를 실행하여 PubMed 데이터 수집, LLM 서버 연결 등 기본 기능이 정상적으로 동작하는지 확인함.

4.  기능 구체화 및 개발
    - `main.py`의 플레이스홀더(Placeholder) 부분을 중심으로 실제 기능을 구현함.
      - a. PDF 다운로드: Unpaywall API 등을 연동하여 PMID나 DOI를 기반으로 PDF 원문을 자동으로 다운로드하는 기능을 `src/ingest`에 추가함.
      - b. 스크리닝 자동화: `ASReview`의 REST API와 연동하여 `main.py`에서 스크리닝 프로젝트를 생성하고, 라벨링 작업을 자동화하는 로직을 구현함.
      - c. 데이터 추출 고도화: `GROBID`로 파싱된 XML에서 텍스트를 추출하고, `LLMClient`를 통해 PICO 정보, 주요 결과 등을 자동으로 추출하여 `data/tables`에 저장하는 기능을 구현함.
      - d. 보고서 생성: 메타분석(R) 및 PRISMA 다이어그램 생성 모듈을 `src/report`에 개발함.

### 03. 프로젝트 구조 개선 및 문서화

1.  `README.md` 작성
    -   프로젝트의 개요, 주요 기능, 구조, 설치 및 사용법을 상세히 기술한 `README.md` 파일을 작성함.

2.  저장소 구조 재구성
    -   깃허브 저장소의 명확성을 높이고 표준적인 구조를 따르기 위해 전체 디렉터리 구조를 재구성함.
    -   핵심 프로젝트인 `sr-gemma3`의 내용을 최상위 디렉터리로 이동시킴.
    -   개발 로그, 메모, 예제 파일 등 참고 자료를 보관하기 위한 `reference_materials` 폴더를 생성하고 관련 파일들을 이전함.
    -   기존의 불필요한 `sr-gemma3` 폴더는 삭제함.

3.  문서 경로 업데이트
    -   구조 변경에 따라, `README.md` 파일 내에 있던 `Development_log.txt`의 경로를 `reference_materials/Development_log.txt`로 수정함.

### 04. PICOS 입력 방식 구현 및 실행 환경 분석

1.  PICOS 질문 입력 로직 구현:
    -   체계적 문헌고찰의 시작점인 PICOS 질문을 입력받는 유연한 방식을 구현함.
    -   `picos_config.yaml` 파일 생성: 연구 질문을 체계적으로 관리하고 재사용할 수 있도록 YAML 설정 파일을 도입함.
    -   `main.py` 로직 수정:
        -   프로그램 실행 시 `picos_config.yaml` 파일이 있으면, 사용자에게 해당 설정을 사용할지 물음.
        -   파일이 없거나 사용자가 원하지 않을 경우, 대화형 프롬프트를 통해 새로운 PICOS 질문을 입력받음.
        -   새로 입력된 내용은 `picos_config.yaml` 파일로 저장(또는 덮어쓰기)할 수 있는 옵션을 제공함.
    -   의존성 추가: YAML 파일 처리를 위해 `requirements.txt`에 `PyYAML` 라이브러리를 추가함.

2.  실행 환경 분석 및 워크플로우 수립:
    -   현재 사용 중인 두 컴퓨터(병원/집)의 하드웨어 스펙을 기반으로 각 파이프라인 단계의 실행 가능성을 분석함.
    -   환경에 무관한 작업: 데이터 수집(PubMed 검색), 문헌 스크리닝(ASReview) 등 전처리 및 상호작용이 필요한 단계를 수행하기에 적합함.
    -   집 컴퓨터 (GPU 기반): LLM(Gemma-3) 모델 추론, RobotReviewer(BERT) 실행, GROBID 대량 처리 등 높은 연산 능력이 요구되는 핵심 AI 작업을 수행하는 데 필수적임.
    -   이에 따라, 병원에서 데이터 수집 및 스크리닝을 진행하고, 집에서 GPU를 활용해 심층 분석을 수행하는 효율적인 분산 워크플로우를 수립함.

### 05. 재사용성을 위한 워크플로우 정립

1.  .gitignore 파일 업데이트:
    -   프로젝트의 재사용성을 높이고 각 연구 프로젝트의 데이터를 분리하기 위해 .gitignore 파일을 수정함.
    -   picos_config.yaml, data/ 폴더, reference_materials/ 폴더를 Git 버전 관리에서 제외하도록 설정함. 이는 깃허브 저장소를 순수한 파이프라인 '템플릿'으로 유지하기 위함임.

2.  서비스 시작 스크립트 (start_services.bat) 생성 및 개선:
    -   GROBID Docker 서비스와 Gemma-3 LLM 서버를 한 번에 쉽게 실행할 수 있도록 start_services.bat 배치 파일을 루트 경로에 생성함.
    -   LLM 모델 파일의 경로를 C:\AI_models\와 같은 절대 경로 대신, 프로젝트 내부의 models/ 폴더를 참조하는 상대 경로로 수정하여 프로젝트의 이동성을 확보함.

3.  다중 주제 연구를 위한 워크플로우 제안:
    -   깃허브 저장소를 파이프라인 템플릿으로 활용하고, 새로운 연구 주제마다 이 템플릿 폴더를 복사하여 독립적인 프로젝트 폴더에서 작업하는 방식을 제안함. 이를 통해 각 연구의 데이터가 템플릿 코드와 분리되어 관리됨.

4.  각 경로별 readme.md 업데이트
    -   빈 폴더가 git에 추적되지 않는 문제를 해결함 
    -   각 폴더별 설명 추가함

##### 2025.10.14 (2일차) 병원 #####

### 06. PDF 자동 다운로드 기능 구현 및 파이프라인 통합

1.  초기 파이프라인 테스트 및 디버깅:
    -   `main.py`의 초기 테스트 과정에서 여러 문제를 발견하고 순차적으로 해결함.
    -   `SyntaxError`: `src/llm/client.py`의 잘못된 문자열 문제를 수정함.
    -   PubMed 검색 오류 (0건 검색):
        -   한글 검색어가 원인임을 파악하고, `picos_config.yaml`의 내용을 영어로 번역하여 문제를 해결함.
        -   이후에도 검색 결과가 없자, 검색어가 너무 구체적이라고 판단하여 더 일반적인 용어로 수정함.
        -   PubMed의 검색 필드 태그 문법에 오류(`:ti,ab`)가 있음을 발견하고, 올바른 문법(`[tiab]`, `[pt]`)을 사용하도록 `main.py`의 `construct_search_query` 함수를 개선함.

2.  PDF 다운로더 모듈 구현:
    -   `향후 작업 계획`에 따라, PDF 자동 다운로드 기능 개발에 착수함.
    -   `src/ingest/downloader.py` 모듈을 새로 생성함.
    -   이 모듈은 `articles.xml` 파일에서 DOI를 추출하고, `Unpaywall API`에 요청을 보내 공개적으로 접근 가능한 PDF 링크를 찾아 로컬(`data/pdf/`)에 저장하는 기능을 포함함.

3.  PubMed API 동작 방식 분석 및 추가 디버깅:
    -   `downloader.py` 테스트 결과, Unpaywall API가 `422 Unprocessable Entity` 오류를 반환하며 다운로드에 실패하는 현상을 확인함.
    -   검색된 논문들이 대부분 아직 정식 출판되지 않은 미래의("in-press") 논문들이라 DOI가 활성화되지 않은 것을 원인으로 추정함.
    -   이 문제를 해결하기 위해, `src/ingest/pubmed.py`의 `fetch_pmids` 함수를 수정하여 날짜 범위(`mindate`, `maxdate`)로 검색을 제한하고 정렬하는 기능을 추가함.
    -   `main.py`에서 이 기능을 사용하도록 수정하여, 과거에 출판된 논문만 검색하도록 설정함.
    -   테스트 결과, 날짜 필터링은 성공했으나 다운로드는 여전히 0건이었음. 이는 검색된 논문들이 오픈 액세스가 아니기 때문이며, 기능 자체는 정상 작동함을 확인함.

4.  파이프라인 통합 및 최종 테스트:
    -   개발 및 테스트가 완료된 `downloader.py` 모듈을 `main.py`의 메인 파이프라인에 통합함.
    -   통합 과정에서 발생한 여러 `IndentationError`를 `main.py` 전체 코드를 재작성하여 해결함.
    -   최종적으로 `main.py`를 실행하여, '논문 검색 -> XML 저장 -> PDF 다운로드 시도'까지의 전체 파이프라인이 오류 없이 한 번에 실행되는 것을 확인하며 기능 구현을 완료함.

### 07. 데이터 초기화 기능 개선 및 CSV 출력 추가

1.  이전 작업 데이터 확인 및 초기화 기능 구현:
    -   `main.py` 실행 시 `data` 폴더에 이전 작업 흔적이 있는지 확인하는 로직을 추가함.
    -   사용자에게 초기화 여부를 묻고, 동의 시 데이터를 정리하도록 구현함.
    -   안전한 데이터 초기화 로직으로 개선: `shutil.rmtree` 대신, `data` 폴더 내의 생성된 파일(`articles.xml`, `retrieved_pmids.csv`, `articles.csv`) 및 `pdf` 폴더의 내용물만 삭제하고, `readme.md`와 같은 비생성 파일 및 디렉터리 구조는 유지하도록 수정함.
    -   데이터 초기화 로직 분리: 이 안전한 초기화 로직을 `src/utils/data_manager.py` 모듈의 `clear_generated_data_files()` 함수로 분리하여 재사용성을 높임.
    -   `main.py`의 `check_and_clear_previous_run()` 함수가 `data_manager.clear_generated_data_files()`를 호출하도록 변경함.

2.  별도 데이터 초기화 스크립트 (`clear_data.py`) 생성:
    -   프로젝트 루트에 `clear_data.py` 스크립트를 생성하여, `python clear_data.py` 명령으로 언제든지 생성된 데이터를 안전하게 초기화할 수 있도록 함.

3.  XML 데이터 CSV 변환 및 저장 기능 추가:
    -   `articles.xml`의 내용을 읽기 쉬운 형태로 제공하기 위해 `src/parse/pubmed_parser.py` 모듈을 새로 생성함.
    -   `pubmed_parser.py`는 PubMed XML에서 PMID, DOI, 제목, 저널, 출판 연도, 초록 등의 핵심 정보를 추출하여 `articles.csv` 파일로 저장하는 기능을 구현함.
    -   `main.py` 파이프라인에 `articles.xml` 저장 직후 `pubmed_parser`를 호출하여 `data/tables/articles.csv`를 생성하는 단계를 추가함.

### 08. 검색 결과 수 사용자 설정 및 미래 논문 필터링 강화

1.  검색 결과 수 사용자 설정 기능 구현:
    -   `main.py`에서 PubMed 검색 시 총 검색 결과 수를 사용자에게 알려주고, 그중 몇 개의 논문을 가져올지 직접 입력받도록 기능을 추가함.
    -   `src/ingest/pubmed.py`의 `fetch_pmids` 함수가 PMIDs 목록과 함께 총 검색 결과 수(`total_count`)를 반환하도록 수정함.
    -   사용자 입력 유효성 검사 로직을 포함하여 안정성을 높임.

2.  미래 출판 예정 논문 필터링 강화:
    -   PubMed API의 `datetype` 필터만으로는 미래 출판 예정 논문(`pub_year`가 현재 연도와 같거나 큰 논문)을 완전히 걸러내기 어려운 문제를 확인.
    -   `main.py`에서 `articles_xml`을 가져온 후, 각 논문의 `pub_year`를 확인하여 현재 연도보다 크거나 같은 논문들을 파이프라인에서 제외하도록 로직을 추가함.
    -   필터링된 XML 데이터만 `pubmed_parser`와 `downloader`에 전달하도록 파이프라인을 개선함.

##### 2025.10.14 (2일차) 집 #####

### 09. 서비스 실행 및 Windows 환경 디버깅

1.  `main.py` 파이프라인 디버깅
    -   문제점: `main.py` 실행 시, `pub_year < current_year` 필터로 인해 2025년에 출판된 모든 논문이 제외되는 문제 발생.
    -   해결책: `main.py`의 연도 필터링 로직을 `pub_year <= current_year`로 수정하여, 현재 연도에 출판된 논문도 포함하도록 변경함.
    -   문제점: PDF 다운로드 단계에서 `downloader.py`가 파일 경로 대신 XML 문자열을 인자로 받아 터미널에 과도한 로그를 출력하는 버그 발견.
    -   해결책: `main.py`에서 `downloader.py`를 호출하는 부분을 수정하여, 저장된 XML 파일의 경로(`xml_path`)를 전달하도록 변경함. 또한, `downloader.py` 내부의 상세 로그를 간결한 요약 정보만 출력하도록 정리함.

2.  `start_services.bat` 실행 환경 디버깅
    -   문제점: `start_services.bat` 실행 시, Windows가 `.llamafile`을 실행 파일로 인식하지 못하고 '연결 프로그램' 대화상자를 띄우는 문제 발생.
    -   해결책: 공식 문서를 통해 Windows에서는 모델 파일의 확장자를 `.exe`로 변경해야 함을 확인. 하지만, `*.exe` 와 `*.llamafile.exe` 사이에서 혼동이 발생함.
    -   문제점: 파일명을 `*.exe`로 변경 후 스크립트를 수정했으나 여전히 파일을 찾지 못함.
    -   해결책: 재확인 결과, 정확한 명명 규칙은 `*.llamafile.exe`임을 확인함. `models` 폴더의 파일명을 `google_gemma-3-12b-it-Q4_K_M.llamafile.exe`로 변경하고, `start_services.bat`가 해당 파일을 참조하도록 최종 수정함.
    -   문제점: 수정된 스크립트를 실행했으나 '액세스가 거부되었습니다' 오류와 함께 Docker 및 LLM 프로세스가 실행되지 않음.
    -   진단: Docker 데몬 접근 및 새 프로세스 실행에 필요한 권한 부족으로 판단됨.
    -   문제점: 관리자 권한으로 실행 시 "이 PC에서는 이 앱을 실행할 수 없습니다" 오류 발생.
    -   진단: 64비트 전용으로 빌드된 LLM 실행 파일을 32비트 Windows에서 실행하려 할 때 발생하는 전형적인 오류로 초기 추정했으나, 사용자의 현재 위치가 고사양 '집 컴퓨터'임을 확인. 따라서 파일 손상 또는 보안 프로그램의 차단일 가능성이 높음.
    -   최종 해결책 제안:
        -   1순위: `models` 폴더의 기존 모델 파일을 삭제하고 Hugging Face 등 신뢰할 수 있는 출처에서 파일을 다시 다운로드하여 정확한 이름(`google_gemma-3-12b-it-Q4_K_M.llamafile.exe`)으로 저장.
        -   2순위: `start_services.bat`를 '관리자 권한으로 실행'.

##### 2025.10.15 (3일차) 집 #####

### 10. LLM 실행 방식 변경: .llamafile에서 Ollama로 전환

1.  `.llamafile` 실행 문제 심층 분석:
    -   문제점: `start_services.bat`를 관리자 권한으로 실행해도 `.llamafile.exe`가 '액세스가 거부되었습니다' 또는 '이 PC에서는 이 앱을 실행할 수 없습니다' 오류를 내며 실행되지 않는 문제가 지속됨.
    -   원인 분석:
        -   1차 원인 (파일 손상): 재 다운로드 후 파일 크기가 비정상적임을 확인, `curl`을 이용해 재시도하여 정상 파일(약 9.5GB)을 확보했으나 여전히 실행 불가.
        -   2차 원인 (Windows 보안 정책): 파일 속성의 '차단 해제' 옵션이 없음을 확인. `takeown`, `icacls`로 파일 권한을 강제 부여해도 실패. 파일을 `C:\llm_test`와 같은 일반 경로로 옮겨 실행해도 실패.
        -   최종 원인 추정: `systeminfo`를 통해 `App Control for Business policy: Enforced` 정책이 활성화된 것을 확인. 이는 허가되지 않은 앱의 실행을 원천적으로 차단하는 강력한 보안 기능으로, 이 정책이 `.llamafile.exe`의 실행을 막고 있는 것으로 최종 결론내림.

2.  대안 수립 및 모델 재선정:
    -   `.llamafile` 방식이 현 PC 환경과 호환되지 않음을 확인하고, 대안으로 Ollama 프레임워크를 도입하기로 결정함. Ollama는 시스템에 정식 설치되는 방식이라 보안 정책을 우회할 수 있고, 안정적인 서비스 운영이 가능함.
    -   PC 사양 재확인: `systeminfo` 및 `Get-CimInstance`를 통해 NVIDIA GeForce RTX 4060 Laptop GPU와 16GB RAM을 확인.
    -   모델 재선정: Gemma 3가 아직 Ollama에 등록되지 않은 점을 고려하여, 현 시점에서 사용 가능하며 원래 계획과 가장 유사한 최신 모델인 `gemma2:9b-instruct`를 최종 선택함. 9B 파라미터 크기는 해당 PC의 GPU/RAM 환경에서 성능과 속도의 최적 균형을 제공할 것으로 기대됨.

3.  향후 작업 계획 (Ollama 기반):
    -   1. Windows용 Ollama를 공식 웹사이트에서 다운로드 및 설치.
    -   2. `ollama pull gemma2:9b-instruct` 명령을 통해 모델 데이터를 다운로드.
    -   3. `start_services.bat`에서 `.llamafile` 실행 부분을 제거하고, Ollama가 시스템 부팅 시 자동으로 시작되므로 별도 실행 로직이 필요 없음을 확인.
    -   4. `src/llm/client.py`를 수정하여, LLM 서버의 `base_url`을 Ollama의 기본 주소(`http://127.0.0.1:11434/v1`)로 변경하고, 요청 모델명을 `gemma2:9b-instruct`로 수정. (완료)
        -   확인: `src/llm/client.py`의 `base_url`이 `http://127.0.0.1:11434/v1`로, 모델명이 `gemma2`로 변경되었음을 확인.
    -   5. `start_services.bat`에서 `.llamafile` 실행 부분을 제거하고, Ollama가 시스템 부팅 시 자동으로 시작되므로 별도 실행 로직이 필요 없음을 확인. (완료)
        -   확인: `start_services.bat`에서 `.llamafile` 관련 코드가 제거되고, Ollama 자동 실행에 대한 메시지로 업데이트되었음을 확인.
    -   6. `main.py` 실행 및 LLM 연결 확인: `main.py`를 실행하여 "LLM client is connected and ready." 메시지를 성공적으로 수신함. (완료)

##### 2025.10.20 (4일차) 집 #####

### 11. PDF 다운로드 로직 심층 분석 및 최종 워크플로우 확립

1.  PDF 다운로드 문제 재확인:
    -   문제점: PubMed 검색 정렬 순서를 최신순(`pub_date`)에서 관련도순(`relevance`)으로 변경했음에도 불구하고, 여전히 Unpaywall API가 `422 Unprocessable Entity` 오류를 반환하며 PDF 다운로드가 0건인 현상 발생.
    -   원인 분석: 단순히 최신 논문이라 DOI가 활성화되지 않은 문제가 아닐 수 있음을 인지하고, 코드 외부에서 API를 직접 호출하여 문제의 원인을 파악하기로 함.

2.  대체 다운로드 전략 수립 및 테스트:
    -   1차 진단 (Unpaywall API 직접 호출): `web_fetch` 도구를 사용해 Unpaywall API를 직접 호출한 결과, 코드와 동일하게 `422` 오류가 반환됨. 이를 통해 파이썬 코드의 요청 방식이 아닌, Unpaywall 서비스가 해당 DOI를 처리할 수 없는 문제임을 확인함.
    -   2차 전략 (PubMed Central 연동): Unpaywall의 대안으로, 논문 데이터에 PMCID(PubMed Central ID)가 있는 경우 PMC에서 직접 PDF를 다운로드하는 로직을 `downloader.py`에 추가함.
    -   2차 진단 (PMC API 직접 호출): PMC 연동 로직 테스트 결과, API가 PDF(`application/pdf`)가 아닌 일반 텍스트(`text/plain`)를 반환하는 문제를 발견. `web_fetch`로 직접 확인한 결과, API가 PDF 대신 논문 요약문을 보내주고 있음을 확인함. 이는 논문의 라이선스 또는 저장 형태의 제약으로 인해 API가 PDF를 제공하지 않는 경우임.

3.  최종 결론 및 워크플로우 확립:
    -   결론: 자동화된 방식(Unpaywall, PMC e-fetch)으로는 현재 검색된 논문들의 PDF를 확보하기 어렵다는 결론에 도달함. 이는 코드의 문제가 아니라, 논문 자체의 라이선스 및 온라인 제공 형태의 한계 때문임.
    -   하이브리드 워크플로우 확립: 파이프라인의 한계를 인정하고, 자동화와 수동 작업을 결합한 현실적인 워크플로우를 최종적으로 확립함.
        -   1. (자동) `main.py`를 실행하여 접근 가능한 모든 오픈 액세스 PDF를 최대한 다운로드.
        -   2. (수동) 다운로드가 실패한 중요 논문은 사용자가 직접 PDF를 다운로드.
        -   3. (수동) 다운로드한 PDF 파일명을 `data/tables/articles.csv`에 있는 PMID에 맞춰 `{PMID}.pdf` 형식으로 변경.
        -   4. (수동) 이름이 변경된 PDF를 `data/pdf/` 폴더에 이동.
        -   5. (자동) `main.py`를 재실행하면, 파이프라인이 수동으로 추가된 PDF를 자동으로 감지하여 이후 단계(GROBID 파싱, LLM 추출)를 수행.
    -   이러한 내용을 `README.md`에 명확히 문서화하여 사용자가 혼동하지 않도록 안내하기로 결정함.