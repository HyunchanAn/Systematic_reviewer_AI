## Systematic review 논문 자동 작성을 위한 파이프라인 구축

##### 2025.10.13 (1일차) 병원 #####

### 00. 사전 조사

    - Gemma-3 모델(.llamafile) 사용법 스터디 노트
    - llamafile 이란?
        - llamafile은 대규모 언어 모델(LLM)의 가중치와 실행 코드를 하나로 묶은 단일 실행 파일임.
        - 덕분에 리눅스, macOS, 윈도우 등 다양한 운영체제에서 별도의 설치 과정 없이 모델을 쉽게 실행할 수 있음.
    - 로컬 모델 실행 방법 (Windows 기준)
        - 로컬에 저장된 모델 파일: `C:\AI_models\google_gemma-3-12b-it-Q4_K_M.llamafile`
        - 기본 실행 (터미널 채팅)
        ```bash
        C:\AI_models\google_gemma-3-12b-it-Q4_K_M.llamafile
        ```
        - GPU 가속 사용 (성능 향상)
        ```bash
        C:\AI_models\google_gemma-3-12b-it-Q4_K_M.llamafile -ngl 999
        ```
        - 웹 UI 사용
        ```bash
        C:\AI_models\google_gemma-3-12b-it-Q4_K_M.llamafile --server
        ```
    - 기타 유용한 옵션
        - 컨텍스트 크기 조절: `-c` 플래그로 컨텍스트 창 크기를 조절할 수 있음. (예: `-c 0`은 최대 크기 사용)
        - 파일과 대화하기: `-f` 플래그로 텍스트 파일을 지정하면, 해당 파일의 내용을 기반으로 대화를 시작할 수 있음.
    - 코드에서 API처럼 사용하기 (Python 예제)
        - `llamafile`을 서버 모드로 실행하면, OpenAI API와 호환되는 로컬 API 서버가 생성됨.
        - 1단계: 서버 실행
        ```bash
        C:\AI_models\google_gemma-3-12b-it-Q4_K_M.llamafile --server -ngl 999
        ```
        - 2단계: Python 코드에서 호출
        ```python
        import openai

        client = openai.OpenAI(
            base_url="http://127.0.0.1:8080/v1",
            api_key="sk-no-key-required"
        )

        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "Hello! Can you tell me about Large Language Models?"}
            ]
        )

        print(completion.choices[0].message.content)
        ```

    - 논문 작성 가이드라인 (Cochrane Intervention Review 기반)
    - 주요 원칙
        - PRISMA 2020 지침 준수
        - 범위 관리: 명확한 연구 질문(PICO) 설정
        - 간결성: 전체 논문 10,000 단어 이내 권장
    - 논문 핵심 구조
        - 1. 초록 (Abstract)
        - 2. 쉬운 말 요약 (Plain Language Summary)
        - 3. 서론 (Background)
        - 4. 연구 방법 (Methods)
        - 5. 연구 결과 (Results)
        - 6. 고찰 (Discussion)
        - 7. 저자 결론 (Authors' conclusion)

    - 데이터 추출 템플릿 구조 (Example001.csv 기반)
    - 목적
        - 체계적 문헌고찰 과정에서 각 논문의 핵심 정보를 일관된 형식으로 추출하고 관리함.
    - 주요 항목 (CSV 컬럼)
        - Title, Author Names, Abstract, DOI, Full Text Link, Embase Link 등

    - 관련 링크
    - Github - Mozilla-Ocho/llamafile: https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file
    - Hugging Face - Mozilla/gemma-3-12b-it-llamafile: https://huggingface.co/Mozilla/gemma-3-12b-it-llamafile
    - How to do a systematic review: https://journals.sagepub.com/doi/full/10.1177/1747493017743796

### 01. 기획 문서 작성 및 초기 환경 구축

    - 프로젝트 구조 생성
    - `sr-gemma3` 프로젝트 디렉터리 구조를 생성함.

    - 오픈소스 도구 복제
    - Git의 긴 파일 경로 문제 해결 (`core.longpaths` 설정) 후, 기획 문서에 명시된 핵심 오픈소스 도구(ASReview, RobotReviewer, GROBID)를 `sr-gemma3/tools/`에 복제함.

    - 의존성 파일 생성
    - Python 의존성 관리를 위한 `requirements.txt` 파일을 생성함 (`openai`, `requests`, `pandas`, `asreview` 등 명시).

    - 초기 코드 스캐폴드 작성
    - SR 파이프라인의 각 기능 모듈에 대한 초기 코드를 작성함.
        - a. `main.py`: 전체 워크플로우를 조율하는 메인 스크립트
        - b. `src/llm/client.py`: 로컬 Gemma 3 (llamafile) 모델과 통신하는 클라이언트
        - c. `src/ingest/pubmed.py`: PubMed API를 통해 논문 데이터를 수집하는 모듈
        - d. `src/parse/grobid_client.py`: PDF 문서를 구조화된 텍스트로 파싱하기 위한 GROBID 클라이언트

### 02. 향후 작업 계획

    - Python 환경 설정
    - `sr-gemma3` 디렉터리에서 가상 환경을 생성하고, `pip install -r requirements.txt` 명령어로 필요한 라이브러리를 설치함.

    - 핵심 서비스 실행
    - a. Docker: Docker Desktop을 설치하고, `sr-gemma3/tools/grobid` 디렉터리의 가이드를 따라 GROBID 서비스를 컨테이너로 실행함.
    - b. LLM 서버: `study_note.txt`를 참고하여 로컬 Gemma 3 모델(`google_gemma-3-12b-it-Q4_K_M.llamafile`)을 GPU 가속을 적용한 서버 모드로 실행함. (`--server -ngl 999`)

    - 초기 파이프라인 테스트
    - `python sr-gemma3/main.py`를 실행하여 PubMed 데이터 수집, LLM 서버 연결 등 기본 기능이 정상적으로 동작하는지 확인함.

    - 기능 구체화 및 개발
    - `main.py`의 플레이스홀더(Placeholder) 부분을 중심으로 실제 기능을 구현함.
        - a. PDF 다운로드: Unpaywall API 등을 연동하여 PMID나 DOI를 기반으로 PDF 원문을 자동으로 다운로드하는 기능을 `src/ingest`에 추가함.
        - b. 스크리닝 자동화: `ASReview`의 REST API와 연동하여 `main.py`에서 스크리닝 프로젝트를 생성하고, 라벨링 작업을 자동화하는 로직을 구현함.
        - c. 데이터 추출 고도화: `GROBID`로 파싱된 XML에서 텍스트를 추출하고, `LLMClient`를 통해 PICO 정보, 주요 결과 등을 자동으로 추출하여 `data/tables`에 저장하는 기능을 구현함.
        - d. 보고서 생성: 메타분석(R) 및 PRISMA 다이어그램 생성 모듈을 `src/report`에 개발함.

### 03. 프로젝트 구조 개선 및 문서화

    - `README.md` 작성
        - 프로젝트의 개요, 주요 기능, 구조, 설치 및 사용법을 상세히 기술한 `README.md` 파일을 작성함.

    - 저장소 구조 재구성
        - 깃허브 저장소의 명확성을 높이고 표준적인 구조를 따르기 위해 전체 디렉터리 구조를 재구성함.
        - 핵심 프로젝트인 `sr-gemma3`의 내용을 최상위 디렉터리로 이동시킴.
        - 개발 로그, 메모, 예제 파일 등 참고 자료를 보관하기 위한 `reference_materials` 폴더를 생성하고 관련 파일들을 이전함.
        - 기존의 불필요한 `sr-gemma3` 폴더는 삭제함.

    - 문서 경로 업데이트
    - 구조 변경에 따라, `README.md` 파일 내에 있던 `Development_log.txt`의 경로를 `reference_materials/Development_log.txt`로 수정함.

### 04. PICOS 입력 방식 구현 및 실행 환경 분석

    - PICOS 질문 입력 로직 구현:
        - 체계적 문헌고찰의 시작점인 PICOS 질문을 입력받는 유연한 방식을 구현함.
        - `picos_config.yaml` 파일 생성: 연구 질문을 체계적으로 관리하고 재사용할 수 있도록 YAML 설정 파일을 도입함.
        - `main.py` 로직 수정:
            - 프로그램 실행 시 `picos_config.yaml` 파일이 있으면, 사용자에게 해당 설정을 사용할지 물음.
            - 파일이 없거나 사용자가 원하지 않을 경우, 대화형 프롬프트를 통해 새로운 PICOS 질문을 입력받음.
            - 새로 입력된 내용은 `picos_config.yaml` 파일로 저장(또는 덮어쓰기)할 수 있는 옵션을 제공함.
        - 의존성 추가: YAML 파일 처리를 위해 `requirements.txt`에 `PyYAML` 라이브러리를 추가함.

    - 실행 환경 분석 및 워크플로우 수립:
        - 현재 사용 중인 두 컴퓨터(병원/집)의 하드웨어 스펙을 기반으로 각 파이프라인 단계의 실행 가능성을 분석함.
        - 환경에 무관한 작업: 데이터 수집(PubMed 검색), 문헌 스크리닝(ASReview) 등 전처리 및 상호작용이 필요한 단계를 수행하기에 적합함.
        - 집 컴퓨터 (GPU 기반): LLM(Gemma-3) 모델 추론, RobotReviewer(BERT) 실행, GROBID 대량 처리 등 높은 연산 능력이 요구되는 핵심 AI 작업을 수행하는 데 필수적임.
        - 이에 따라, 병원에서 데이터 수집 및 스크리닝을 진행하고, 집에서 GPU를 활용해 심층 분석을 수행하는 효율적인 분산 워크플로우를 수립함.

### 05. 재사용성을 위한 워크플로우 정립

    - .gitignore 파일 업데이트:
        - 프로젝트의 재사용성을 높이고 각 연구 프로젝트의 데이터를 분리하기 위해 .gitignore 파일을 수정함.
        - picos_config.yaml, data/ 폴더, reference_materials/ 폴더를 Git 버전 관리에서 제외하도록 설정함. 이는 깃허브 저장소를 순수한 파이프라인 '템플릿'으로 유지하기 위함임.

        - 서비스 시작 스크립트 (start_services.bat) 생성 및 개선:
            - GROBID Docker 서비스와 Gemma-3 LLM 서버를 한 번에 쉽게 실행할 수 있도록 start_services.bat 배치 파일을 루트 경로에 생성함.
            - LLM 모델 파일의 경로를 C:\AI_models\와 같은 절대 경로 대신, 프로젝트 내부의 models/ 폴더를 참조하는 상대 경로로 수정하여 프로젝트의 이동성을 확보함.

    - 다중 주제 연구를 위한 워크플로우 제안:
        - 깃허브 저장소를 파이프라인 템플릿으로 활용하고, 새로운 연구 주제마다 이 템플릿 폴더를 복사하여 독립적인 프로젝트 폴더에서 작업하는 방식을 제안함. 이를 통해 각 연구의 데이터가 템플릿 코드와 분리되어 관리됨.

    - 각 경로별 readme.md 업데이트
        - 빈 폴더가 git에 추적되지 않는 문제를 해결함 
        - 각 폴더별 설명 추가함

##### 2025.10.14 (2일차) 병원 #####

### 06. PDF 자동 다운로드 기능 구현 및 파이프라인 통합

    - 초기 파이프라인 테스트 및 디버깅:
        - `main.py`의 초기 테스트 과정에서 여러 문제를 발견하고 순차적으로 해결함.
        - `SyntaxError`: `src/llm/client.py`의 잘못된 문자열 문제를 수정함.
        - PubMed 검색 오류 (0건 검색):
            - 한글 검색어가 원인임을 파악하고, `picos_config.yaml`의 내용을 영어로 번역하여 문제를 해결함.
            - 이후에도 검색 결과가 없자, 검색어가 너무 구체적이라고 판단하여 더 일반적인 용어로 수정함.
            - PubMed의 검색 필드 태그 문법에 오류(`:ti,ab`)가 있음을 발견하고, 올바른 문법(`[tiab]`, `[pt]`)을 사용하도록 `main.py`의 `construct_search_query` 함수를 개선함.

    - PDF 다운로더 모듈 구현:
        - `향후 작업 계획`에 따라, PDF 자동 다운로드 기능 개발에 착수함.
        - `src/ingest/downloader.py` 모듈을 새로 생성함.
        - 이 모듈은 `articles.xml` 파일에서 DOI를 추출하고, `Unpaywall API`에 요청을 보내 공개적으로 접근 가능한 PDF 링크를 찾아 로컬(`data/pdf/`)에 저장하는 기능을 포함함.

    - PubMed API 동작 방식 분석 및 추가 디버깅:
        - `downloader.py` 테스트 결과, Unpaywall API가 `422 Unprocessable Entity` 오류를 반환하며 다운로드에 실패하는 현상을 확인함.
        - 검색된 논문들이 대부분 아직 정식 출판되지 않은 미래의("in-press") 논문들이라 DOI가 활성화되지 않은 것을 원인으로 추정함.
        - 이 문제를 해결하기 위해, `src/ingest/pubmed.py`의 `fetch_pmids` 함수를 수정하여 날짜 범위(`mindate`, `maxdate`)로 검색을 제한하고 정렬하는 기능을 추가함.
        - `main.py`에서 이 기능을 사용하도록 수정하여, 과거에 출판된 논문만 검색하도록 설정함.
        - 테스트 결과, 날짜 필터링은 성공했으나 다운로드는 여전히 0건이었음. 이는 검색된 논문들이 오픈 액세스가 아니기 때문이며, 기능 자체는 정상 작동함을 확인함.

    - 파이프라인 통합 및 최종 테스트:
        - 개발 및 테스트가 완료된 `downloader.py` 모듈을 `main.py`의 메인 파이프라인에 통합함.
        - 통합 과정에서 발생한 여러 `IndentationError`를 `main.py` 전체 코드를 재작성하여 해결함.
        - 최종적으로 `main.py`를 실행하여, '논문 검색 -> XML 저장 -> PDF 다운로드 시도'까지의 전체 파이프라인이 오류 없이 한 번에 실행되는 것을 확인하며 기능 구현을 완료함.

### 07. 데이터 초기화 기능 개선 및 CSV 출력 추가

    - 이전 작업 데이터 확인 및 초기화 기능 구현:
        - `main.py` 실행 시 `data` 폴더에 이전 작업 흔적이 있는지 확인하는 로직을 추가함.
        - 사용자에게 초기화 여부를 묻고, 동의 시 데이터를 정리하도록 구현함.
        - 안전한 데이터 초기화 로직으로 개선: `shutil.rmtree` 대신, `data` 폴더 내의 생성된 파일(`articles.xml`, `retrieved_pmids.csv`, `articles.csv`) 및 `pdf` 폴더의 내용물만 삭제하고, `readme.md`와 같은 비생성 파일 및 디렉터리 구조는 유지하도록 수정함.
        - 데이터 초기화 로직 분리: 이 안전한 초기화 로직을 `src/utils/data_manager.py` 모듈의 `clear_generated_data_files()` 함수로 분리하여 재사용성을 높임.
        - `main.py`의 `check_and_clear_previous_run()` 함수가 `data_manager.clear_generated_data_files()`를 호출하도록 변경함.

    - 별도 데이터 초기화 스크립트 (`clear_data.py`) 생성:
        - 프로젝트 루트에 `clear_data.py` 스크립트를 생성하여, `python clear_data.py` 명령으로 언제든지 생성된 데이터를 안전하게 초기화할 수 있도록 함.

    - XML 데이터 CSV 변환 및 저장 기능 추가:
        - `articles.xml`의 내용을 읽기 쉬운 형태로 제공하기 위해 `src/parse/pubmed_parser.py` 모듈을 새로 생성함.
        - `pubmed_parser.py`는 PubMed XML에서 PMID, DOI, 제목, 저널, 출판 연도, 초록 등의 핵심 정보를 추출하여 `articles.csv` 파일로 저장하는 기능을 구현함.
        - `main.py` 파이프라인에 `articles.xml` 저장 직후 `pubmed_parser`를 호출하여 `data/tables/articles.csv`를 생성하는 단계를 추가함.

### 08. 검색 결과 수 사용자 설정 및 미래 논문 필터링 강화

    - 검색 결과 수 사용자 설정 기능 구현:
        - `main.py`에서 PubMed 검색 시 총 검색 결과 수를 사용자에게 알려주고, 그중 몇 개의 논문을 가져올지 직접 입력받도록 기능을 추가함.
        - `src/ingest/pubmed.py`의 `fetch_pmids` 함수가 PMIDs 목록과 함께 총 검색 결과 수(`total_count`)를 반환하도록 수정함.
        - 사용자 입력 유효성 검사 로직을 포함하여 안정성을 높임.

    - 미래 출판 예정 논문 필터링 강화:
        - PubMed API의 `datetype` 필터만으로는 미래 출판 예정 논문(`pub_year`가 현재 연도와 같거나 큰 논문`)을 완전히 걸러내기 어려운 문제를 확인.
        - `main.py`에서 `articles_xml`을 가져온 후, 각 논문의 `pub_year`를 확인하여 현재 연도보다 크거나 같은 논문들을 파이프라인에서 제외하도록 로직을 추가함.
        - 필터링된 XML 데이터만 `pubmed_parser`와 `downloader`에 전달하도록 파이프라인을 개선함.

##### 2025.10.14 (2일차) 집 #####

### 09. 서비스 실행 및 Windows 환경 디버깅

    - `main.py` 파이프라인 디버깅
        - 문제점: `main.py` 실행 시, `pub_year < current_year` 필터로 인해 2025년에 출판된 모든 논문이 제외되는 문제 발생.
        - 해결책: `main.py`의 연도 필터링 로직을 `pub_year <= current_year`로 수정하여, 현재 연도에 출판된 논문도 포함하도록 변경함.
        - 문제점: PDF 다운로드 단계에서 `downloader.py`가 파일 경로 대신 XML 문자열을 인자로 받아 터미널에 과도한 로그를 출력하는 버그 발견.
        - 해결책: `main.py`에서 `downloader.py`를 호출하는 부분을 수정하여, 저장된 XML 파일의 경로(`xml_path`)를 전달하도록 변경함. 또한, `downloader.py` 내부의 상세 로그를 간결한 요약 정보만 출력하도록 정리함.

    - `start_services.bat` 실행 환경 디버깅
        - 문제점: `start_services.bat` 실행 시, Windows가 `.llamafile`을 실행 파일로 인식하지 못하고 '연결 프로그램' 대화상자를 띄우는 문제 발생.
        - 해결책: 공식 문서를 통해 Windows에서는 모델 파일의 확장자를 `.exe`로 변경해야 함을 확인. 하지만, `*.exe` 와 `*.llamafile.exe` 사이에서 혼동이 발생함.
        - 문제점: 파일명을 `*.exe`로 변경 후 스크립트를 수정했으나 여전히 파일을 찾지 못함.
        - 해결책: 재확인 결과, 정확한 명명 규칙은 `*.llamafile.exe`임을 확인함. `models` 폴더의 파일명을 `google_gemma-3-12b-it-Q4_K_M.llamafile.exe`로 변경하고, `start_services.bat`가 해당 파일을 참조하도록 최종 수정함.
        - 문제점: 수정된 스크립트를 실행했으나 '액세스가 거부되었습니다' 오류와 함께 Docker 및 LLM 프로세스가 실행되지 않음.
        - 진단: Docker 데몬 접근 및 새 프로세스 실행에 필요한 권한 부족으로 판단됨.
        - 문제점: 관리자 권한으로 실행 시 "이 PC에서는 이 앱을 실행할 수 없습니다" 오류 발생.
        - 진단: 64비트 전용으로 빌드된 LLM 실행 파일을 32비트 Windows에서 실행하려 할 때 발생하는 전형적인 오류로 초기 추정했으나, 사용자의 현재 위치가 고사양 '집 컴퓨터'임을 확인. 따라서 파일 손상 또는 보안 프로그램의 차단일 가능성이 높음.
        - 최종 해결책 제안:
            - 1순위: `models` 폴더의 기존 모델 파일을 삭제하고 Hugging Face 등 신뢰할 수 있는 출처에서 파일을 다시 다운로드하여 정확한 이름(`google_gemma-3-12b-it-Q4_K_M.llamafile.exe`)으로 저장.
            - 2순위: `start_services.bat`를 '관리자 권한으로 실행'.

##### 2025.10.15 (3일차) 집 #####

### 10. LLM 실행 방식 변경: .llamafile에서 Ollama로 전환

    - `.llamafile` 실행 문제 심층 분석:
        - 문제점: `start_services.bat`를 관리자 권한으로 실행해도 `.llamafile.exe`가 '액세스가 거부되었습니다' 또는 '이 PC에서는 이 앱을 실행할 수 없습니다' 오류를 내며 실행되지 않는 문제가 지속됨.
        - 원인 분석:
            - 1차 원인 (파일 손상): 재 다운로드 후 파일 크기가 비정상적임을 확인, `curl`을 이용해 재시도하여 정상 파일(약 9.5GB)을 확보했으나 여전히 실행 불가.
            - 2차 원인 (Windows 보안 정책): 파일 속성의 '차단 해제' 옵션이 없음을 확인. `takeown`, `icacls`로 파일 권한을 강제 부여해도 실패. 파일을 `C:\llm_test`와 같은 일반 경로로 옮겨 실행해도 실패.
            - 최종 원인 추정: `systeminfo`를 통해 `App Control for Business policy: Enforced` 정책이 활성화된 것을 확인. 이는 허가되지 않은 앱의 실행을 원천적으로 차단하는 강력한 보안 기능으로, 이 정책이 `.llamafile.exe`의 실행을 막고 있는 것으로 최종 결론내림.

    - 대안 수립 및 모델 재선정:
        - `.llamafile` 방식이 현 PC 환경과 호환되지 않음을 확인하고, 대안으로 Ollama 프레임워크를 도입하기로 결정함. Ollama는 시스템에 정식 설치되는 방식이라 보안 정책을 우회할 수 있고, 안정적인 서비스 운영이 가능함.
        - PC 사양 재확인: `systeminfo` 및 `Get-CimInstance`를 통해 NVIDIA GeForce RTX 4060 Laptop GPU와 16GB RAM을 확인.
        - 모델 재선정: Gemma 3가 아직 Ollama에 등록되지 않은 점을 고려하여, 현 시점에서 사용 가능하며 원래 계획과 가장 유사한 최신 모델인 `gemma2:9b-instruct`를 최종 선택함. 9B 파라미터 크기는 해당 PC의 GPU/RAM 환경에서 성능과 속도의 최적 균형을 제공할 것으로 기대됨.

    - 향후 작업 계획 (Ollama 기반):
        - 1. Windows용 Ollama를 공식 웹사이트에서 다운로드 및 설치.
        - 2. `ollama pull gemma2:9b-instruct` 명령을 통해 모델 데이터를 다운로드.
        - 3. `start_services.bat`에서 `.llamafile` 실행 부분을 제거하고, Ollama가 시스템 부팅 시 자동으로 시작되므로 별도 실행 로직이 필요 없음을 확인.
        - 4. `src/llm/client.py`를 수정하여, LLM 서버의 `base_url`을 Ollama의 기본 주소(`http://127.0.0.1:11434/v1`)로 변경하고, 요청 모델명을 `gemma2:9b-instruct`로 수정. (완료)
            - 확인: `src/llm/client.py`의 `base_url`이 `http://127.0.0.1:11434/v1`로, 모델명이 `gemma2`로 변경되었음을 확인.
        - 5. `start_services.bat`에서 `.llamafile` 실행 부분을 제거하고, Ollama가 시스템 부팅 시 자동으로 시작되므로 별도 실행 로직이 필요 없음을 확인. (완료)
            - 확인: `start_services.bat`에서 `.llamafile` 관련 코드가 제거되고, Ollama 자동 실행에 대한 메시지로 업데이트되었음을 확인.
        - 6. `main.py` 실행 및 LLM 연결 확인: `main.py`를 실행하여 "LLM client is connected and ready." 메시지를 성공적으로 수신함. (완료)

##### 2025.10.20 (4일차) 집 #####

### 11. PDF 다운로드 로직 심층 분석 및 최종 워크플로우 확립

    - PDF 다운로드 문제 재확인:
        - 문제점: PubMed 검색 정렬 순서를 최신순(`pub_date`)에서 관련도순(`relevance`)으로 변경했음에도 불구하고, 여전히 Unpaywall API가 `422 Unprocessable Entity` 오류를 반환하며 PDF 다운로드가 0건인 현상 발생.
        - 원인 분석: 단순히 최신 논문이라 DOI가 활성화되지 않은 문제가 아닐 수 있음을 인지하고, 코드 외부에서 API를 직접 호출하여 문제의 원인을 파악하기로 함.

    - 대체 다운로드 전략 수립 및 테스트:
        - 1차 진단 (Unpaywall API 직접 호출): `web_fetch` 도구를 사용해 Unpaywall API를 직접 호출한 결과, 코드와 동일하게 `422` 오류가 반환됨. 이를 통해 파이썬 코드의 요청 방식이 아닌, Unpaywall 서비스가 해당 DOI를 처리할 수 없는 문제임을 확인함.
        - 2차 전략 (PubMed Central 연동): Unpaywall의 대안으로, 논문 데이터에 PMCID(PubMed Central ID)가 있는 경우 PMC에서 직접 PDF를 다운로드하는 로직을 `downloader.py`에 추가함.
        - 2차 진단 (PMC API 직접 호출): PMC 연동 로직 테스트 결과, API가 PDF(`application/pdf`)가 아닌 일반 텍스트(`text/plain`)를 반환하는 문제를 발견. `web_fetch`로 직접 확인한 결과, API가 PDF 대신 논문 요약문을 보내주고 있음을 확인함. 이는 논문의 라이선스 또는 저장 형태의 제약으로 인해 API가 PDF를 제공하지 않는 경우임.

    - 최종 결론 및 워크플로우 확립:
        - 결론: 자동화된 방식(Unpaywall, PMC e-fetch)으로는 현재 검색된 논문들의 PDF를 확보하기 어렵다는 결론에 도달함. 이는 코드의 문제가 아니라, 논문 자체의 라이선스 및 온라인 제공 형태의 한계 때문임.
        - 하이브리드 워크플로우 확립: 파이프라인의 한계를 인정하고, 자동화와 수동 작업을 결합한 현실적인 워크플로우를 최종적으로 확립함.
            - 1. (자동) `main.py`를 실행하여 접근 가능한 모든 오픈 액세스 PDF를 최대한 다운로드.
            - 2. (수동) 다운로드가 실패한 중요 논문은 사용자가 직접 PDF를 다운로드.
            - 3. (수동) 다운로드한 PDF 파일명을 `data/tables/articles.csv`에 있는 PMID에 맞춰 `{PMID}.pdf` 형식으로 변경.
            - 4. (수동) 이름이 변경된 PDF를 `data/pdf/` 폴더에 이동.
            - 5. (자동) `main.py`를 재실행하면, 파이프라인이 수동으로 추가된 PDF를 자동으로 감지하여 이후 단계(GROBID 파싱, LLM 추출)를 수행.
        - 이러한 내용을 `README.md`에 명확히 문서화하여 사용자가 혼동하지 않도록 안내하기로 결정함.

##### 2025.10.22 (5일차) 집 #####

### 12. GROBID 연동 및 TEI-XML 파싱 구현

    - PDF 파싱 파이프라인 통합:
        - `main.py`에 PDF 다운로드 단계 이후, `data/pdf` 폴더 내의 모든 PDF 파일을 GROBID 서비스로 전송하는 로직을 추가함.
        - `src/parse/grobid_client.py`의 `process_pdfs_in_directory` 함수를 호출하여 PDF를 처리하고, 파싱된 TEI-XML 파일을 `data/tei` 폴더에 저장하도록 구현함.
        - `src/parse/tei_parser.py` 모듈을 사용하여 TEI-XML에서 논문의 주요 섹션(초록, 서론, 방법, 결과 등) 텍스트를 추출하는 기능을 구현함.

    - LLM 기반 정보 추출 로직 추가:
        - `tei_parser`에서 추출된 텍스트를 `src/llm/client.py`를 통해 Ollama의 `gemma2:9b-instruct` 모델로 전송하는 로직을 `main.py`에 통합함.
        - LLM에 PICO 요소(Population, Intervention, Comparison, Outcome) 및 주요 결과(Key Findings)를 추출하도록 지시하는 프롬프트 엔지니어링을 적용함.
        - LLM으로부터 추출된 구조화된 데이터는 `data/tables/extracted_data.csv` 파일로 저장되도록 구현함.

    - 수정된 파일:
        - `main.py`: GROBID 및 LLM 호출 파이프라인 통합.
        - `src/parse/grobid_client.py`: PDF 파일명 처리 로직 개선.
        - `src/parse/tei_parser.py`: TEI-XML 파싱 및 텍스트 추출 로직 구현.
        - `src/llm/client.py`: LLM 프롬프트 정의 및 응답 처리 로직 추가.

    - 기능 테스트 및 검증:
        - `start_services.bat`를 통해 GROBID Docker 컨테이너가 실행 중임을 확인.
        - `data/pdf` 폴더에 샘플 PDF 파일 3개를 배치.
        - `python main.py`를 실행하여 전체 파이프라인을 구동.
        - `data/tei` 폴더에 3개의 TEI-XML 파일이 정상적으로 생성되었는지 확인.
        - `data/tables/extracted_data.csv` 파일이 생성되었는지, 그리고 PICO 요소 및 주요 결과가 올바르게 추출되었는지 수동으로 검토.

    - 발견된 문제 및 해결:
        - 문제점: PDF 파일명에 공백이나 특수문자가 포함된 경우 GROBID API 호출 시 오류가 발생함.
        - 해결책: `grobid_client.py`에서 PDF 파일을 GROBID로 전송하기 전에 파일명을 URL-safe한 형태로 임시 변경하도록 처리 로직을 추가하여 해결함.

### 13. GROBID 파싱 오류 진단 및 환경 문제 확인

    - GROBID 파싱 오류 발생:
        - `main.py` 실행 시 GROBID 파싱 단계에서 `HTTP ERROR 500 Internal Server Error`가 지속적으로 발생함.
        - GROBID 컨테이너 로그에서 `java.lang.NullPointerException: null` 오류가 `org.grobid.core.utilities.IOUtilities.writeInputFile`에서 반복적으로 발생함을 확인. 이는 GROBID가 PDF를 처리하는 과정에서 내부 파일 쓰기 작업에 실패했음을 의미함.

    - 문제 해결 시도 및 결과:
        - `src/parse/grobid_client.py` 수정: PDF 파일명을 URL-safe하게 전송하도록 코드 수정. -> 문제 해결 안 됨.
        - `tools/grobid/docker-compose.yml` 수정: GROBID 컨테이너의 메모리 제한을 4GB로 증설. -> 문제 해결 안 됨.
        - Docker Desktop 공장 초기화 및 재설치: Docker Desktop의 WSL 2 통합 문제 해결을 위해 재설치 진행. -> 문제 해결 안 됨.

    - WSL 환경 진단:
        - `wsl --list --verbose` 명령 실행 시 `docker-desktop-data` 배포판이 지속적으로 누락됨을 확인. 이는 Docker Desktop이 컨테이너 이미지 및 데이터를 저장하는 핵심 WSL 배포판이 생성되지 않았음을 의미함.
        - `wsl --status`, `wsl --version` 명령 및 Windows 기능(Linux용 Windows 하위 시스템, 가상 머신 플랫폼) 활성화 여부 확인 결과, WSL 자체의 기본적인 설정은 정상임을 확인.

    - 결론:
        - GROBID 파싱 오류는 코드 또는 GROBID 설정 문제가 아닌, 사용자 시스템의 Docker Desktop 및 WSL 2 환경 문제로 판단됨.
        - `docker-desktop-data` 배포판이 정상적으로 프로비저닝되지 않는 것이 근본 원인으로 추정됨.

### 14. OpenAI codex를 이용한 troubleshooting 내용 

    - 요약
        - 현재 시스템에서는 wsl -l -q에 docker-desktop만 보이고, 별도의 docker-desktop-data 배포판은 등록되지 않았습니다.
        - 그러나 Docker 백엔드 로그와 WSL 디스크를 보면 엔진은 정상 동작 중이며, 데이터 VHDX가 %LOCALAPPDATA%\Docker\wsl 아래에 존재합니다. 이는 최신 Docker Desktop에서 데이터 디스크를 별도 WSL 배포판(docker-desktop-data)으로 등록하지 않고, 단일 배포판(docker-desktop)과 로컬 VHDX로 관리하는 변경 사항일 가능성이 큽니다.

    - 증상 정리
        - WSL 배포판 목록: docker-desktop만 표시되고 docker-desktop-data는 없음.
        - 데이터 디스크 파일 존재: C:\Users\chema\AppData\Local\Docker\wsl\disk\docker_data.vhdx, C:\Users\chema\AppData\Local\Docker\wsl\main\ext4.vhdx
        - Docker 엔진은 동작 중: C:\Users\chema\AppData\Local\Docker\log\host\com.docker.backend.exe.log

    - 원인 가능성
        - 최신 Docker Desktop(WSL 엔진 구조 업데이트)에서는 docker-desktop-data를 별도 WSL 배포판으로 등록하지 않고, docker-desktop 한 개만 보이며 데이터는 VHDX로 관리됩니다. 이 경우 “docker-desktop-data가 안 보인다”는 것이 문제 상황이 아닐 수 있습니다.
        - 실제로 프로비저닝 실패가 있는 경우의 공통 원인:
            - WSL2/가상화 기능 미활성화 또는 WSL 커널 구버전
            - %LOCALAPPDATA%\Docker\wsl 폴더의 잔여/손상 파일(부분 생성된 data/main 디렉터리, 손상된 VHDX)
            - 디스크 공간 부족(C: 드라이브 여유 공간 부족)
            - 권한/백신 간섭(프로필/OneDrive 리디렉션 포함)

    - 우선 비파괴 점검
        - WSL/버전 상태 확인: wsl --status, wsl --version, wsl --set-default-version 2
        - 배포판 확인: wsl -l -q (실제 배포판 목록), wsl --list --running
        - 여유 공간 확인(최소 수 GB 이상 권장)
        - 로그에서 에러 키워드 검색(등록 실패 등): C:
            \Users\chema\AppData\Local\Docker\log\host\com.docker.backend.exe.log
        - VHDX 존재 여부 확인:
            - C:\Users\chema\AppData\Local\Docker\wsl\disk\docker_data.vhdx
            - C:\Users\chema\AppData\Local\Docker\wsl\main\ext4.vhdx

    - 현재 장비에서는 위 파일들이 정상 존재하고, 로그에도 등록 실패 흔적이 없어 “구조 변경으로 인한 오해”일 가능성이 높습니다.

    - 실제 프로비저닝 실패일 때 해결책
        - 기본 복구(권장 순서):
            1. WSL 업데이트: wsl --update
            2. WSL 전체 종료: wsl --shutdown
            3. Docker Desktop 완전 종료
            4. Docker Desktop 재실행(필요시 관리자 권한) → 자동 재프로비저닝 유도
        - 그래도 실패 시(파괴적, 데이터 소실 주의):
            - 로컬 이미지/볼륨이 삭제됩니다. 중요 데이터는 컨테이너에서 docker save/docker export 등으로 백업하세요.
                1. wsl --shutdown
                2. 잔여 배포판 제거: wsl --unregister docker-desktop (있으면), wsl --unregister docker-desktop-data (있으면)
                3. 폴더 정리: - %LOCALAPPDATA%\Docker\wsl 전체 삭제(또는 이름 변경 후 백업)
                4. Docker Desktop 재시작 → 배포판/디스크 재생성
        - 환경 이슈 완화:
            - Docker Desktop 설정에서 디스크 이미지 위치를 시스템 루트 근처로 이동(예: C:\Docker\wsl\data)해 OneDrive/권한 이슈 회피
            - Windows 기능 확인(WSL, Virtual Machine Platform), BIOS 가상화 활성화
            - 엔터프라이즈 백신/정책 제외 목록에 %LOCALAPPDATA%\Docker 추가

        - 검증
            - wsl -l -q에서 최신 구조는 docker-desktop 하나만 보일 수 있습니다. 이 상태에서도 엔진/이미지/볼륨이 정상 동작하면 문제 없습니다.
            - Docker Desktop 대시보드에서 이미지/컨테이너 조회, 간단한 docker run hello-world로 확인.

### 15. chatGPT 5를 이용한 문제 원인 추측

    - 저 에러는 “클라이언트가 뭔가 잘못했다”보다는, GROBID 서비스 내부에서 예외가 터져서(500) 생기는 경우가 대부분입니다. 아래 체크리스트를 윗줄부터 순서대로 진행하면 빠르게 원인을 특정할 수 있습니다.

    1. 서버 상태/라우팅 문제
        - 헬스체크: GET http://<host>:8070/api/isalive → 200이 떠야 정상. 404/502/타임아웃이면 라우팅/프록시 문제.
        - 엔드포인트: 풀텍스트는 POST /api/processFulltextDocument 가 맞습니다(오타·경로 확인).

    2. 요청 형식 문제 (가장 흔함)
        - multipart 필드명: 파일 파라미터는 input 여야 합니다.
        - 잘못된 키(예: file, pdf)로 올리면 500이 날 수 있습니다.
        - 샘플 cURL(확실한 최소 재현)
        ```
        curl -v -X POST "http://<host>:8070/api/processFulltextDocument" \
        -F "input=@31241782.pdf" \
        -F "consolidateCitations=1" \
        -F "teiCoordinates=persName,figDesc" \
        -F "segmentSentences=1"
        ```
        - 위가 통과하면, 당신의 코드에서 multipart 구성/헤더가 잘못된 것입니다.
        - Content-Type은 multipart/form-data 이고, 개별 파트에 별도 Content-Type: application/pdf 를 붙여도 되고 생략해도 됩니다. 핵심은 -F input=@....

    3. 입력 파일 이슈
        - 암호화/스캔 PDF: 암호화되어 있거나 이미지 스캔만 있는 PDF에서 pdfalto 단계가 실패 → 500.
            → 다른 “정상 텍스트 PDF 2~3쪽짜리”로 테스트.
        - 파손/비표준 PDF: pdffonts/pdftotext로 한 번 떠보면 구조 이상 여부를 빨리 알 수 있습니다.
        - 파일 크기 제한: 리버스 프록시(Nginx)의 client_max_body_size, Jetty 폼 사이즈 제한에 걸리면 500/413.
            → Nginx 앞단이면 client_max_body_size 100m; 등으로 상향, GROBID 쪽은 Request.maxFormContentSize 조정 고려.

    4. 런타임/환경 문제
    - pdfalto 미설치/경로: 로컬 빌드 배포에서 pdfalto 경로가 안 잡히면 500.
    - Docker 이미지는 포함되어 있지만, 베어메탈이면 pdfalto 설치 + GROBID가 찾을 수 있는 PATH 필요.
    - 모델/리소스 경로: grobid-home 경로가 잘못되거나 모델 다운로드 미완료 시 예외.
    - 첫 실행 직후 모델 받는 동안 실패 → 재기동 후 재시도.
    - JVM 메모리 부족(OOM): 큰 PDF 대량 처리 시 흔함.
        - JAVA_OPTS="-Xms1g -Xmx4g -XX:+UseG1GC" 같이 힙을 넉넉히. Docker면 -e JAVA_OPTS=....
    - 동시 처리 과다: 기본 워커 스레드 수 대비 과도한 동시 요청 → 큐 적체/타임아웃 후 500. 동시성 낮춰서 재시도해보세요.

    5. 서버 로그로 즉시 단정
    - 서비스 로그 확인(가장 결정적)
        - Docker: docker logs <grobid_container> -f
        - 로컬: grobid-service/logs/*.log
        - 스택트레이스에 FileNotFoundException pdfalto, OutOfMemoryError, InvalidPdfException 등 키워드가 바로 찍힙니다.

    6. 축소 재현으로 분기
    - GET /api/isalive 확인
    - 위의 샘플 cURL로 2~3쪽짜리 정상 PDF 테스트
        - 성공 시: 클라이언트 코드 multipart/헤더/프록시 문제
        - 실패 시: 서버 환경(pdfalto, 모델, 메모리) 또는 업로드 제한
    - /api/processHeaderDocument 로 같은 파일 테스트
        - 헤더는 되고 Fulltext만 실패면 본문 레이아웃 파싱(pdfalto) 단계에서 터진 것.

    7. Python(requests) 최소 코드 예시

    ```
    import requests

    url = "http://<host>:8070/api/processFulltextDocument"
    files = {"input": open("31241782.pdf", "rb")}
    data = {
        "consolidateCitations": "1",
        "segmentSentences": "1",
        "teiCoordinates": "persName,figDesc"
    }
    r = requests.post(url, files=files, data=data, timeout=300)
    print(r.status_code)
    print(r.headers.get("content-type"))
    print(r.text[:1000])
    ```

    - files 키가 input 인지, data는 문자열인지(정수 넣어도 되나 문자열 권장) 확인.
    - timeout 충분히.

    8. 프록시/쿠버네티스 배포 시 추가 체크
    - Nginx/Ingress: client_max_body_size, 업스트림 타임아웃(proxy_read_timeout) 상향.
    - 쿠버네티스 리소스: Pod 메모리/CPU 리밋 상향, readinessProbe/livenessProbe 실패 여부.
    - 파일 업로드가 1~2분 걸릴 정도면 업스트림 타임아웃 먼저 의심.

    9. 결론
    - 가장 빠른 진단은 서버 로그와 정상 PDF로의 cURL 최소 재현입니다.
    - 로그에 OOM/경로/pdfalto/모델 관련 메시지가 있으면 환경 문제, 없고 cURL은 되는데 애플리케이션만 실패하면 multipart 키/헤더/프록시 설정 문제입니다.
    - 스캔/암호화/비표준 PDF라면 processHeaderDocument는 되고 Fulltext만 500인 패턴이 흔합니다.

### 16. GROBID 파싱 오류 진단 및 해결 

    - curl http://127.0.0.1:8070/api/isalive - true 결과 출력. 서비스는 정상 실행중이며 외부에서 접근 가능한 상태
    
    - 파이썬 코드를 거치지 않고 curl 명령을 통해 PDF 파일을 직접 GROBID에 전송하여 문제의 원인이 코드에 있는지, GROBID 서비스에 있는지 판별 
        - curl -v -X POST "http://127.0.0.1:8070/api/processFulltextDocument" -F "input=@C:\Users\chema\Github\Systematic_reviewer_AI\data\pdf\\31241782.pdf
        - 이 테스트에서 HTTP 200 OK 응답과 함께 성공적으로 TEI-XML 결과가 반환됨. 이는 매우 중요한 결과임. 
            1. GROBID Docker 컨테이너는 정상적으로 작동하고 있음 
            2. Docker와 WSL 2 환경 설정에는 문제 없음
            3. GROBID 서비스 자체는 PDF 파일을 올바르게 처리할 수 있음 
            4. 따라서 문제의 원인은 src/parse/grobid_client.py 파일 내부에 있음. 
    
    - multipart/form-data 요청에 대해 서버가 기대하는 필드 이름(input)과 다른 이름을 사용하기 때문일 것으로 추측함 
        - process_pdf 함수에서 GROBID 서버로 파일을 보낼 때, input 대신 inputFile 이라는 잘못된 필드명을 사용하고 있었음. 
        - files = {'inputFile': (clean_filename, f, 'application/pdf;)} 부분에서 서버가 파일을 제대로 받지 못해 500 Internal Server Error를 반환하고 있었음. 
        - 따라서 위 코드에서 inputFile 을 input으로 수정함. 
    
    - 이로서 GROBID 연동 문제가 해결됨. 

### 17. Screening 단계 개발 
    
    - ASReview REST API 이해
        - 아직 개발중(완전히 문서화되지 않음): ASReview LAB 내부에서 사용되고 있음 
        - ASReview LAB에서는 모델, 데이터 및 프로젝트 관리를 위한 광범위한 기능을 제공하는 문서화된 저수준 python API를 제공한다고 명시됨 
        - 따라서 REST API 대신 Python API를 사용하여 ASReview를 통합하기로 함.
        - 공식문서 확인
            - ASReview Read the Docs
            - ASReview Software Documentation (Zenodo) 

    - ASReview에 대한 정보 요약
        - systematic review 프로젝트를 시작하고, 데이터 세트를 가져오고, 스크리닝을 위해 AI를 사용하는 데 설계되었음을 확인하는 높은 수준의 개요를 제공
        - asreview.nl 링크가 일반적인 개요를 제공했으나, Read the Docs 링크에서 더 구체적인 정보를 제공함.
        - 정보는 asreview.nl(메인 웹사이트), zenodo.org(소프트웨어 문서), readthedocs.io(블로그)를 참조함. 
        - https://asreview.readthedocs.io/en/latest/
            - ASReview LAB 가이드, 서버 가이드, 기술 가이드 총 3가지로 구분되어 작성됨 
            - 기술 가이드: 아키텍처, 명령줄 사용 및 확장 개발, ASReview 파일에서 데이터 액세스 방법에 대한 세부 정보를 제공함.
            - ASReview LAB 가이드: AI 모델 및 검토 관리가 언급되어 있음 (데이터 가져오기 및 프로젝트 생성에 대한 정보가 포함될 것으로 보임)

    - 목표가 반드시 검토를 시뮬레이션하는 것은 아니지만 위 사이트에서 아래와 같은 스크립트 예제를 제공함. 
        1. CSV에서 데이터 가져오기: dataset 매개변수는 CSV 파일의 경로를 가져옵니다. 여기에 articles.csv를 사용할 수 있음
        2. ASReview 프로젝트 파일 생성: output_file 매개변수는 .asreview 프로젝트 파일이 저장될 위치를 지정
        3. 능동 학습 모델 구성: model, query_strategy, feature_extractor, balance_strategy와 같은 매개변수를 설정할 수 있음 

    - 가능성 평가
        - 핵심은 asreview.simulate.run()을 사용하여 CSV에서 ASReview 프로젝트를 생성하고 능동 학습 프로세스를 설정할 수 있음
        - 개발 로그에는 "ASReview의 REST API와 연동하여 main.py에서 스크리닝 프로젝트를 생성하고, 라벨링 작업을 자동화하는 로직을 구현함"이라고 언급되어 있음. 
        - 이 시뮬레이션 함수는 적어도 프로젝트 및 초기 능동 학습을 설정하는 프로그래밍 방식으로 추정됨 
        - 그러나 이 예제는 시뮬레이션 목적(정답)을 위해 CSV에 included 열을 사용함
        - 현재 articles.csv에는 이것이 구현되어있지 않음.
        - 실제 스크리닝 프로젝트의 경우 일반적으로 미리 레이블이 지정된 데이터 없이 시작하거나 사용 가능한 경우 소수의 미리 레이블이 지정된 논문으로 시작함 

    - main.py에 ASReview를 통합하기 위한 업데이트된 계획은 다음과 같습니다.

        1. `main.py`를 `asreview.simulate.run()`을 사용하도록 수정:
            - main.py에 ASReview 통합을 위한 새 단계를 추가
            - articles.csv를 입력 dataset으로 사용
            - ASReview 프로젝트의 output_file을 정의(예: data/asreview_project.asreview).
            - 통합 작업을 수행하는 데 중점을 두므로 현재는 기본 모델 매개변수 사용
            - n_prior_included 및 n_prior_excluded 매개변수를 처리하는 방법을 고려해야 함
            - 새로운 스크리닝의 경우 이들은 0일 수 있으며, 사용 가능한 경우 소수의 미리 레이블이 지정된 논문을 제공해야 할 수도 있음(현재는 0으로 설정)

        2. "라벨링 작업 자동화" 부분 고려: asreview.simulate.run() 함수는 시뮬레이션을 위한 것임을 확인함
            - 실제 대화형 스크리닝 및 "라벨링 작업 자동화"의 경우 일반적으로 ASReview LAB(UI)이 사용됨
            - 개발 로그에는 "라벨링 작업을 자동화하는 로직을 구현함"이라고 언급되어 있음
            - 이는 ASReview의 능동 학습 기능을 사용하여 레이블을 제안하는 것을 의미할 수 있지만, 사용자는 여전히 이를 확인해야 한다고 명시됨 
            - 레이블링의 완전한 자동화가 필요한 경우 ASReview의 내부 상태 또는 사용자 지정 능동 학습 루프와의 더 고급 상호 작용이 필요할 수 있으며, 이는 간단한 simulate.run()의 범위를 벗어날 것으로 보임.
            - 현재는 프로젝트를 설정하고 ASReview LAB에서 스크리닝을 준비하는 데 중점을 둠.

    - ASReview 통합 로직 개발 
        1. `asreview` 가져오기: 파일 상단에 import asreview를 추가합니다.
        2. ASReview 프로젝트 경로 정의: ASReview 프로젝트 파일에 대한 상수 정의(예: ASREVIEW_PROJECT_PATH = os.path.join(DATA_DIR, "asreview_project.asreview")).
        3. `main()`에 ASReview 통합:
            - ASReview 단계 전에 articles.csv가 존재하고 최신 상태인지 확인
            - csv_path(os.path.join(TABLES_DIR, "articles.csv"))를 dataset으로, ASREVIEW_PROJECT_PATH를 output_file로 사용하여 asreview.simulate.run() 호출
            - n_prior_included 및 n_prior_excluded는 현재 0으로 설정 (새로운 스크리닝을 시작하기 때문)
            - ASReview 프로젝트를 진행하는 방법에 대한 메시지를 사용자에게 추가(예: ASReview LAB UI에서 열기)
        4. import os, import pandas as pd, import yaml, import shutil을 main.py 맨 위에 추가함. 
        5. 그러나 replace 작업이 제대로 적용되지 않았거나 old_string이 정확히 일치하지 않는 것으로 보임.
        
    - troubleshooting 
        1. asreview.simulate.run()을 사용하여 articles.csv 데이터에서 ASReview 프로젝트 파일(data/asreview_project.asreview)을 생성 
        2. main.py에서 ASReview 가져오기 문을 import asreview에서 from asreview import simulate로 수정함; module 'asreview' has no attribute 'simulate' 오류 해결하기 위함 
        
### 18. 실험 중 발견한 추가적인 문제
    - step 5에서 CPU가 아니라 GPU를 사용해야 하나, CPU 연산량만 급증함. 이는 step 4 구현 이후 해결할 예정. 
    
    - 예상 가능한 문제 원인 
        1. Ollama 구성:
            - `start_services.bat`: 개발 로그에는 llamafile에 대해 C:\AI_models\google_gemma-3-12b-it-Q4_K_M.llamafile --server -ngl 999가 언급되었고 나중에 Ollama로 전환됨 
            - Ollama의 경우 GPU 오프로딩은 일반적으로 Ollama 자체에서 처리
            - `ollama pull gemma2:9b-instruct`: 모델을 가져올 때 Ollama는 GPU를 감지했어야 함 
            - Ollama 서버 시작: Ollama는 일반적으로 백그라운드 서비스로 시작
            - GPU를 사용하도록 구성되어 있는지 확인해야 함 

        2. Ollama GPU 감지/구성:
            - NVIDIA 드라이버: NVIDIA GPU 드라이버가 최신 상태인지 확인합니다.
            - CUDA/cuDNN: Ollama는 NVIDIA GPU용 CUDA에 의존함. Ollama는 일반적으로 필요한 것을 번들로 제공하지만 드라이버 호환성이 중요함
            - Ollama 로그: Ollama 로그에서 GPU 감지 또는 시작 중 오류와 관련된 메시지를 확인합니다.
                - 일반적으로 Ollama 설치 디렉토리에서 찾거나 터미널에서 ollama serve를 실행하여 출력을 관찰할 수 있음 
            - Ollama 환경 변수: 표준 설정에서는 덜 일반적이지만 OLLAMA_GPU 또는 OLLAMA_NUM_GPU와 같은 환경 변수가 필요할 수 있음 

        3. 모델 호환성:
            - gemma2:9b-instruct는 GPU 호환성을 위해 선택되었지만, 일부 모델 또는 양자화 수준은 GPU로 완전히 오프로드되지 않을 수 있음 

        4. `src/llm/client.py`:
            - Python 클라이언트(src/llm/client.py)는 API를 통해 Ollama 서버와 상호 작용
            - GPU 사용량을 직접 제어하지 않고 서버에 요청을 보낼 뿐입니다. 서버(Ollama)는 GPU를 사용하는 역할

    - 문제 해결 단계:

        1. Ollama GPU 구성 확인:
            - Ollama 다시 시작: Ollama가 서비스로 실행 중인 경우 다시 시작하는 것으로 감지 문제가 해결될 수 있음 
            - 새 터미널에서 `ollama run gemma2:9b-instruct` 실행: 출력 관찰
                - Ollama는 모델이 로드될 때 GPU 감지 및 사용에 대한 메시지를 인쇄하는 경우가 많습니다.
            - Ollama의 `config.json` 확인(존재하는 경우): GPU 관련 설정 탐색 
                - 위치는 다를 수 있지만 C:\Users\<YourUser>\.ollama\config.json 또는 이와 유사한 곳에 있는 경우가 많음 
            - 작업 관리자/GPU 모니터 확인
                - Ollama가 요청을 처리하는 동안(main.py의 5단계 동안) 작업 관리자(또는 더 자세한 GPU 모니터링 도구)를 열어 GPU 사용량이 약간이라도 급증하는지 확인합니다.

        2. `start_services.bat`가 간섭하지 않는지 확인:
            - start_services.bat가 GPU 사용을 허용하는 방식으로 Ollama를 올바르게 시작하는지 확인
            - .llamafile 부분을 제거하고 Ollama가 자동으로 시작되게 했음 
            - Ollama를 수동으로 시작하는 경우 올바르게 수행되었는지 확인해야 함 

        3. Ollama를 사용한 최소 테스트:
            - 새 터미널을 열어 ollama run gemma2:9b-instruct를 실행
            - "안녕하세요, 이름이 무엇입니까?"와 같은 간단한 프롬프트를 입력하고 답변 확인 
            - 응답이 생성되는 동안 GPU 사용량을 관찰합니다. 
                - 이 간단한 작업에도 GPU 활동이 없다면 문제는 main.py 스크립트가 아니라 Ollama 설정 또는 GPU 드라이버에 있는 것으로 간주


##### 2025.10.23 (6일차) 집 #####

### 19. ASReview 연동 의존성 문제 해결 및 문서 업데이트

    - ASReview 통합 오류 발생:
        - `main.py` 실행 시 `ModuleNotFoundError: No module named 'asreview.entry_points'` 오류가 발생함.
        - 기존 로그를 참고하여 `from asreview import simulate`로 수정했으나, `ImportError: cannot import name 'simulate'`라는 새로운 오류가 발생하며 API 경로가 잘못되었음을 확인함.

    - 의존성 문제 진단 및 해결:
        - 웹 검색을 통해 ASReview의 시뮬레이션 기능이 별도의 확장(extension) 패키지를 통해 제공될 가능성을 확인함.
        - `asreview-simulation`, `asreview-contrib-simulation` 등 여러 패키지 이름을 시도했으나 `pip`이 패키지를 찾지 못하는 오류가 발생함.
        - PyPI(Python Package Index)에서 공식 확장 기능 목록을 재검색하여, 시뮬레이션 워크플로우와 관련된 `asreview-makita`가 올바른 패키지임을 최종적으로 확인함.
        - `requirements.txt`의 패키지 이름을 `asreview-makita`로 수정한 뒤, `pip install -r requirements.txt`를 통해 성공적으로 의존성을 설치함.
        - `main.py`의 import 구문을 `from asreviewcontrib.simulation.api import run as asreview_run`으로, 함수 호출을 `asreview_run()`으로 수정하여 코드와 실제 라이브러리 구조를 일치시킴.

    - 문서 업데이트:
        - 문제 해결 과정을 반영하여 `README.md` 파일을 업데이트함.
        - '주요 기능'의 스크리닝 섹션을 '플레이스홀더' 상태에서, `asreview-makita`를 활용한 자동 프로젝트 생성 기능으로 상세히 설명함.
        - `README.md`의 '개발 로그' 섹션에도 ASReview 연동 디버깅 과정을 요약하여 추가함.
